{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO9Er5JNbqOrJVIvbXj0tRG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"PSQ_acD43hQ4","executionInfo":{"status":"ok","timestamp":1687141270149,"user_tz":-330,"elapsed":3725,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch.autograd import grad"]},{"cell_type":"markdown","source":["## 1st method using grad()"],"metadata":{"id":"g54DWwlWkdai"}},{"cell_type":"code","source":["x1 = torch.tensor(2.0, requires_grad = True)\n","x1.grad, x1.requires_grad\n","\n","# y = x1**2\n","y1 = torch.cos(x1)\n","y2 = torch.sin(x1)\n","y = y1+y2\n","# print(y1, y1.grad)\n","# print(y2, y2.grad)\n","# print(y, y.grad)\n","\n","print('--------------')\n","print(y.requires_grad, x1.grad)\n","# 1st way\n","\n","gradients = grad( outputs = y, inputs=x1 )\n","print(gradients)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNS5Ed8SgQ2c","executionInfo":{"status":"ok","timestamp":1687059045161,"user_tz":-330,"elapsed":2,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"d49fc9e9-5795-4aaa-8a0f-6b32a2ee8cb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------\n","True None\n","(tensor(-1.3254),)\n"]}]},{"cell_type":"code","source":["# internal calculation of gradient\n","y1_grad = -torch.sin(x1)\n","y2_grad = torch.cos(x1)\n","y1_grad + y2_grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rXYTBcSbkJXk","executionInfo":{"status":"ok","timestamp":1687059311888,"user_tz":-330,"elapsed":3,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"00a6a271-98bb-486e-e8e6-4af6c021fc9b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-1.3254, grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# 2nd attempt of the above code.\n","\n","x1 = torch.tensor(3.0, requires_grad=True)\n","# x1.grad, x1.requires_grad\n","y = x1**2\n","# y = 3**2 = 9\n","print(y, y.requires_grad)\n","\n","y2 = torch.sin(x1)\n","print(y2, y2.requires_grad)\n","# as soon as you perform some OP on tensor, the operation that was performed gets stored for future reference, a.k.a for backprop.\n","\n","y3 = torch.mean(y2)\n","print(y3, y3.requires_grad)\n","\n","# no gradient\n","x2 = torch.tensor(3.0)\n","y4 = torch.sin(x2)\n","print(y4, y4.requires_grad)\n","# requires_grad=True, PyTorch creates DAG, and stores the operations.\n","\n","\n","\n","# Gradient computation\n","x1 = torch.tensor(3.0, requires_grad=True)\n","y1 = torch.sin(x1)\n","\n","gradients = grad(outputs=y1, inputs=x1) #dy1/dx1\n","print(gradients)\n","\n","# gradients_2 = grad(grad(outputs=y1, inputs=x1, create_graph=True))\n","# print(gradients_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"32C9eAFEdNpH","executionInfo":{"status":"ok","timestamp":1687141973924,"user_tz":-330,"elapsed":424,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"8bd3771c-84b0-4ef5-ae02-60a616343678"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(9., grad_fn=<PowBackward0>) True\n","tensor(0.1411, grad_fn=<SinBackward0>) True\n","tensor(0.1411, grad_fn=<MeanBackward0>) True\n","tensor(0.1411) False\n","(tensor(-0.9900),)\n"]}]},{"cell_type":"markdown","source":["## 2nd Method using backward()"],"metadata":{"id":"U9J2Rlikkaoj"}},{"cell_type":"code","source":["x1 = torch.tensor(2.0, requires_grad = True)\n","x1.grad, x1.requires_grad\n","\n","# y = x1**2\n","y1 = torch.cos(x1)\n","# y1.retain_grad() use this if you would like calculate dy/dy1.\n","y2 = torch.sin(x1)\n","# y2.retain_grad() use this if you would like calculate dy/dy2\n","\n","\n","y = y1+y2 # 2(y1+y2)\n","\n","# print(y1.grad)\n","\n","y.backward(retain_graph=True)  #asking pytorch debugger to look at the \"grad_fn of y\" and then compute the derivative.\n","print(f\"Gradient single derivative: {x1.grad}\") #gives the 1st derivative, because we have used .backward once.\n","# print(x1.grad, y1.grad, y2.grad) # accessing dy/dx\n","# y1.grad -> dy/dy1 = (y1 + y2)/dy1 = 2.\n","# y2.grad -> dy/dy2 = (y1 + y2)/dy2 = 2.\n","\n","\n","# what if I can use .backward() once again??\n","# does using .backward() again give me 2nd derivative\n","# twice the .backward() -> d2y/dx2 ??\n","#  solution: To use .backward() twice/>twice, make sure the computation graph is not destroyed. so use retain_graph=True while computing 1st derivative.\n","# y.backward(retrain_graph=Tru)\n","# print(x1.grad)\n","\n","for i in range(10):\n","  y.backward(retain_graph=True)\n","  print(x1.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ewj9PJsEkYnz","executionInfo":{"status":"ok","timestamp":1687060549256,"user_tz":-330,"elapsed":2,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"192adc86-2a61-4762-cd24-42aba32f6456"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient single derivative: -1.325444221496582\n","tensor(-2.6509)\n","tensor(-3.9763)\n","tensor(-5.3018)\n","tensor(-6.6272)\n","tensor(-7.9527)\n","tensor(-9.2781)\n","tensor(-10.6036)\n","tensor(-11.9290)\n","tensor(-13.2544)\n","tensor(-14.5799)\n"]}]},{"cell_type":"code","source":["x1 = torch.tensor(2.0, requires_grad = True)\n","x1.grad, x1.requires_grad\n","\n","# y = x1**2\n","y1 = torch.cos(x1)\n","# y1.retain_grad() use this if you would like calculate dy/dy1.\n","y2 = torch.sin(x1)\n","# y2.retain_grad() use this if you would like calculate dy/dy2\n","\n","\n","y = y1+y2 # 2(y1+y2)\n","\n","\n","# gradients = grad(outputs=y, inputs=[x1])\n","y.backward(retain_graph=True)\n","print(x1.grad)\n","\n","# 2nd derivative\n","y.backward(retain_graph=True)\n","print(x1.grad)\n","\n","# 3rd deriv\n","y.backward()\n","print(x1.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYFmd3E1gGI0","executionInfo":{"status":"ok","timestamp":1687142266862,"user_tz":-330,"elapsed":4,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"269ae4a1-e49b-48fc-e710-6e103cd6738f"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(-1.3254)\n","tensor(-2.6509)\n","tensor(-3.9763)\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vH8Tql6xgJTM","executionInfo":{"status":"ok","timestamp":1687142147812,"user_tz":-330,"elapsed":728,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"36447cb7-dd0a-4dea-c101-12f64ae78e95"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(-1.3254)\n"]}]},{"cell_type":"code","source":["# # internally,\n","# -torch.sin(x1) + torch.cos(x1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"okMejZ17gIys","executionInfo":{"status":"ok","timestamp":1687142131818,"user_tz":-330,"elapsed":434,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"9198ca62-9f7a-42f3-d90f-0ecf74f9353a"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-1.3254, grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["## Multi Dims"],"metadata":{"id":"s17sr7cphHAz"}},{"cell_type":"code","source":["# R1-> R1\n","x = torch.tensor(2.0, requires_grad=True)\n","y = 2*x\n","print(y)\n","y.backward() #dy/dx\n","print(x.grad)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BfNqC4O_uBFs","executionInfo":{"status":"ok","timestamp":1687142349841,"user_tz":-330,"elapsed":4,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"0a916587-feb8-4f78-d018-afd34e4913d0"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(4., grad_fn=<MulBackward0>)\n","tensor(2.)\n"]}]},{"cell_type":"code","source":["# RN -> R1\n","x = torch.tensor([2.0, 2.0, 2.0],requires_grad=True)\n","def operation(x):\n","  val = 2*x\n","  val = torch.cumsum(val, dim=0)\n","  return val[-1]\n","\n","y = operation(x)\n","print(y)\n","\n","y.backward()\n","print(x.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0BXWNZyNhSsO","executionInfo":{"status":"ok","timestamp":1687142478991,"user_tz":-330,"elapsed":4,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"33f3eb4f-4f1b-407d-dc9e-eb5e94182a69"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(12., grad_fn=<SelectBackward0>)\n","tensor([2., 2., 2.])\n"]}]},{"cell_type":"code","source":["x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n","\n","def operation(x):\n","  val = 2*x[0] + 3*x[1] -3*x[2]\n","  return val\n","\n","y = operation(x)\n","print(y)\n","\n","y.backward()\n","print(x.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J7jRcKRaiK9b","executionInfo":{"status":"ok","timestamp":1687142662469,"user_tz":-330,"elapsed":5,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"3dcaaccd-fc32-4b0c-efad-5086e60628d0"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(-1., grad_fn=<SubBackward0>)\n","tensor([ 2.,  3., -3.])\n"]}]},{"cell_type":"markdown","source":["## Issue! Things get a bit dramatic, as soon as you have vectors"],"metadata":{"id":"DxIoecmtlIgA"}},{"cell_type":"code","source":["# Rn -> Rm (n,m > 1)\n","\n","x = torch.tensor([1.0, 2.0, -2.0], requires_grad=True)\n","print(x.grad)\n","# y1 = torch.sin(x1)\n","y1 = x**2\n","print(y1)\n","print(x.grad)\n","# y2 = torch.cos()\n","\n","\n","# we want gradient -> [dy1/dx1, dy1/dx2, dy1/dx3]\n","# y1.backward([1.0, 0.0, 0.0])\n","# print(x.grad)\n","\n","# convert vector to scalar\n","# y = [y1, y2, y3]\n","\n","input_key = torch.tensor([[1.0, 0.0, 0.0],\n","                          [0.0, 1.0, 0.0],\n","                          [0.0, 0.0, 1.0]])\n","\n","# final_gradient = []\n","\n","# for row in input_key:\n","#   print(row)\n","#   gradient = y1.backward(row, retain_graph=True)\n","#   final_gradient.append(gradient)\n","\n","\n","# print(final_gradient)\n","# print(x.grad)\n","# tensor([ 2.,  4., -4.]) -> 1st method\n","\n","\n","# 2nd method\n","y1.backward(torch.tensor([1.0, 1.0, 1.0]))\n","print(x.grad)\n","# tensor([ 2.,  4., -4.])\n","\n","# we can use .backward(torch.tensor(1.0, 0.0, 1.0)) -> in this case, only gradients of x1, x3 will be computed.\n","# Also, we can make use of .backward(torch.tensor(1.0, 0.8, 1.8)) API to do gradient scaling/weightage -> x1 100, x2 80%, x3 180%\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gOhgTjuEmBT0","executionInfo":{"status":"ok","timestamp":1687062755343,"user_tz":-330,"elapsed":2,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"fc356d22-0baa-4584-8660-113318573dd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n","tensor([1., 4., 4.], grad_fn=<PowBackward0>)\n","None\n","tensor([ 2.0000,  3.2000, -2.4000])\n"]}]},{"cell_type":"code","source":["# 2nd attempt\n","# Rn -> Rm (n,m > 1)\n","\n","x = torch.tensor([1.0, 2.0, -2.0], requires_grad=True)\n","print(x.grad)\n","# y1 = torch.sin(x1)\n","y1 = x**2\n","print(y1)\n","print(x.grad)\n","\n","# backprop\n","# y1.backward()\n","# print(x.grad)\n","\n","# Let's compute just 1st row\n","# key = torch.tensor([1.0, 0.0, 0.0])\n","# y1.backward(key)\n","# print(x.grad)\n","\n","\n","# 1st way\n","# y1.backward(torch.tensor([1, 1, 1]))\n","# print(x.grad)\n","\n","#2d way\n","input_key = torch.tensor([[1.0, 0.0, 0.0],\n","                          [0.0, 1.0, 0.0],\n","                          [0.0, 0.0, 1.0]])\n","\n","final_jacobian = []\n","for key in input_key:\n","\n","  x.grad = None\n","  gradients = y1.backward(key, retain_graph=True)\n","\n","  final_jacobian.append(x.grad)\n","\n","print(final_jacobian)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IWZc0cY4jPbf","executionInfo":{"status":"ok","timestamp":1687143949384,"user_tz":-330,"elapsed":473,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"1416c74a-2334-4ce2-cf99-ba81750e18c3"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n","tensor([1., 4., 4.], grad_fn=<PowBackward0>)\n","None\n","[tensor([2., 0., -0.]), tensor([0., 4., -0.]), tensor([ 0.,  0., -4.])]\n"]}]},{"cell_type":"code","source":["\n","# create tensors with requires_grad = true\n","x = torch.tensor(3.0)\n","# x = np.array([3.0])\n","# whenever you create a tensor with required_grad = True, the .grad is empty.\n","# print(x.grad)\n","y = torch.tensor(4.0)\n","\n","# print the tensors\n","print(\"x:\", x)\n","print(\"y:\", y)\n","\n","# define a function z of above created tensors\n","z = x*y\n","print(\"z:\", z)\n","\n","# call backward function for z to compute the gradients\n","# dz/dx, dz/dy -> z.backward()\n","# z.backward(retain_graph=True)\n","#  z = x*y ; DAG gets destroyed.\n","\n","# # Access and print the gradients w.r.t x, and y\n","# dx = x.grad\n","# dy = y.grad\n","# # x.grad = 0\n","# # y.grad = 0\n","# # z.backward(retain_graph=True)\n","# z.backward(retain_graph=True)\n","# print(\"x.grad :\", dx)\n","# print(\"y.grad :\", dy)\n","# # pytorch accumulates the gradients.\n","# z.backward()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lO4OuXAY3udU","executionInfo":{"status":"ok","timestamp":1667193033926,"user_tz":-330,"elapsed":374,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"6fe54775-af06-46a4-e958-9318e9a7e7e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x: tensor(3.)\n","y: tensor(4.)\n","z: tensor(12.)\n"]}]},{"cell_type":"code","source":["x = torch.tensor(3.0, requires_grad= True)\n","y = torch.tensor(4.0, requires_grad = True)\n","\n","z = x*y\n","print(x.grad, y.grad)\n","h = z.mean()\n","print(z)\n","print(h)\n","z.backward()\n","print(x.grad, y.grad)\n","\n","\n","x1 = torch.tensor([1.2, 2.4, 3.3], requires_grad = True)\n","x2 = torch.tensor([4.4, 5.2, 5.5], requires_grad=True)\n","f = x1*x2 + torch.sin(x2) + torch.log(x1)\n","g = f.mean()\n","print(f, g)\n","f.backward(gradient=torch.tensor([1., 1., 1.]))\n","# g.backward()\n","print(x1.grad, x2.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cy1G_7mfdFTW","executionInfo":{"status":"ok","timestamp":1667193958042,"user_tz":-330,"elapsed":372,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"d89aa0ef-82e9-46b0-9af3-d5fd050b7587"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["None None\n","tensor(12., grad_fn=<MulBackward0>)\n","tensor(12., grad_fn=<MeanBackward0>)\n","tensor(4.) tensor(3.)\n","tensor([ 4.5107, 12.4720, 18.6384], grad_fn=<AddBackward0>) tensor(11.8737, grad_fn=<MeanBackward0>)\n","tensor([5.2333, 5.6167, 5.8030]) tensor([0.8927, 2.8685, 4.0087])\n"]}]},{"cell_type":"code","source":["# create tensors with requires_grad = true\n","x = torch.tensor(3.0, requires_grad = True)\n","y = torch.tensor(4.0, requires_grad = True)\n","\n","# print the tensors\n","print(\"x:\", x)\n","print(\"y:\", y)\n","\n","# define a function z of above created tensors\n","z = x*y\n","print(\"z:\", z)\n","# z.retain_graph = True\n","# call backward function for z to compute the gradients\n","dx2 = grad(z, x, retain_graph=True)\n","dy2 = grad(z, y)\n","# Access and print the gradients w.r.t x, and y\n","# dx = x.grad\n","# dy = y.grad\n","print(\"x.grad :\", dx2)\n","print(\"y.grad :\", dy2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XowJrRac4Ahr","executionInfo":{"status":"ok","timestamp":1666587596166,"user_tz":-330,"elapsed":361,"user":{"displayName":"Rohith marktricks","userId":"06455389861066924786"}},"outputId":"4feb84b1-93c4-4978-a7b0-db94555d876e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x: tensor(3., requires_grad=True)\n","y: tensor(4., requires_grad=True)\n","z: tensor(12., grad_fn=<MulBackward0>)\n","x.grad : (tensor(4.),)\n","y.grad : (tensor(3.),)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"AlUiahL44OFb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k0gVV-r-4OBE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5DI4xW8W4N87"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gpxwfzYf4N1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XeYpsevH4RYi"},"execution_count":null,"outputs":[]}]}